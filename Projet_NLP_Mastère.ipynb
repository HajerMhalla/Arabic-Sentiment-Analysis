{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet NLP Mastère.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### *Import  libraries*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hMZOYBfoqNJl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "KBbczyHUkK2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cefe9fd-eb5b-4e76-f775-4d936c2b7ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from keras.backend import clear_session\n",
        "from keras.models import load_model\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import string\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Import data and label encoding* "
      ],
      "metadata": {
        "id": "GOPZ-zdLDbo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/gdrive/MyDrive/corpusAAS.txt',sep='\\n', names=['sent'])\n",
        "data['y'] = data.sent.str[-3:]\n",
        "data['sent'] = data.sent.str[:-4]\n",
        "\n",
        "data['y'].replace('pos', 1, inplace=True)\n",
        "data['y'].replace('neg', 0, inplace=True)"
      ],
      "metadata": {
        "id": "udllYcQrrui4"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head(2000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "OYM7U6yesMvD",
        "outputId": "5eb471ff-0858-402d-8c84-8ccdb7b4a1f3"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4f7736c8-f8f0-4db6-af6e-a5d518b833da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>'طوال حياتي لم المس اي تغير حتى قدمت هذه الحكو...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>'منتوج رائع  وثمن مناسب ....جميل'</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'كلنا ابن كيران لمتافق معايا يدير جيم'</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>'وفقك الله لولاية اخرى حقاش مكينش محسن منك'</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>'لأنه و بكل بساطة رئيس الحكومة يعتني بمعاق داخ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>'اصمت لعلى صمتك راحة بالنسبة لهم'</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>'حديقة حيوانات ولازال هنالك اناس لا يؤمنون بنظ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>'أفعى بجدارة تريثت تربصت وكان الفحيح متعة له ص...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>'لا يقطع الرأس غير الي ركبه الان اصبح تركيب ال...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>'امة النون نستنكر ندين نشجب ثم نوافق'</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f7736c8-f8f0-4db6-af6e-a5d518b833da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f7736c8-f8f0-4db6-af6e-a5d518b833da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f7736c8-f8f0-4db6-af6e-a5d518b833da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                   sent  y\n",
              "0     'طوال حياتي لم المس اي تغير حتى قدمت هذه الحكو...  1\n",
              "1                     'منتوج رائع  وثمن مناسب ....جميل'  1\n",
              "2                'كلنا ابن كيران لمتافق معايا يدير جيم'  1\n",
              "3           'وفقك الله لولاية اخرى حقاش مكينش محسن منك'  1\n",
              "4     'لأنه و بكل بساطة رئيس الحكومة يعتني بمعاق داخ...  1\n",
              "...                                                 ... ..\n",
              "1995                  'اصمت لعلى صمتك راحة بالنسبة لهم'  0\n",
              "1996  'حديقة حيوانات ولازال هنالك اناس لا يؤمنون بنظ...  0\n",
              "1997  'أفعى بجدارة تريثت تربصت وكان الفحيح متعة له ص...  0\n",
              "1998  'لا يقطع الرأس غير الي ركبه الان اصبح تركيب ال...  0\n",
              "1999              'امة النون نستنكر ندين نشجب ثم نوافق'  0\n",
              "\n",
              "[2000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ar_stopwords =\"نفسنا مثل حيث ذلك بشكل لدى ألا عن إلي ب لنا وقالت فقط الذي الذى ا هذا غير أكثر اي أنا أنت ايضا اذا كيف وكل أو اكثر أي أن منه وكان وفي تلك إن سوف حين نفسها هكذا قبل حول منذ هنا عندما على ضمن لكن فيه عليه قليل صباحا لهم بان يكون بأن أما هناك مع فوق بسبب ما لا هذه و فيها ف ولم ل آخر ثانية انه من الان جدا به بن بعض حاليا بها هم أ كانت هي لها نحن تم أنفسهم ينبغي إلى فان وقد تحت عند وجود الى فأن الي او قد خارج إنه اى مرة هؤلاء أنها إذا فهي فهى كل يمكن جميع أنفسكم فعل كان ثم لي الآن وقال فى في ديك لم لن له تكون الذين ليس التى التي أنه وان بعد حتى ان دون وأن لماذا يجري كلا إنها لك ضد وإن فهو انها منها أى لديه ولا بين خلال وما اما عليها بعيدا كما نفسي نحو هو نفسك نفسه انت ولن إضافي لقاء وكانت هى فما أيضا إلا معظم ومن إما الا بينما وهي وهو وهى\""
      ],
      "metadata": {
        "id": "f16BpixxtRes"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ar_stopwords=nltk.word_tokenize(ar_stopwords)\n"
      ],
      "metadata": {
        "id": "nDHr9hiA4CXB"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A function to clean and preprocess data"
      ],
      "metadata": {
        "id": "FTDpfWwjD107"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):  \n",
        "\n",
        "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\n",
        "              \"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
        "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\n",
        "               \"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ', ' ! ']\n",
        "    #remove diacritic\n",
        "    diacritic = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
        "    text = re.sub(diacritic,\"\", text)\n",
        "  \n",
        "    longation = re.compile(r'(.)\\1+')\n",
        "    subst = r\"\\1\\1\"\n",
        "    \n",
        "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
        "    #remove english words\n",
        "    text = re.sub(r\"[a-zA-Z]\", '', text)\n",
        "    #remove spaces\n",
        "    text = re.sub(r\"\\d+\", ' ', text)\n",
        "    text = re.sub(r\"\\n+\", ' ', text)\n",
        "    text = re.sub(r\"\\t+\", ' ', text)\n",
        "    text = re.sub(r\"\\r+\", ' ', text)\n",
        "    text = re.sub(r\"\\s+\", ' ', text)\n",
        "    #remove repetetions\n",
        "    text = text.replace('وو', 'و')\n",
        "    text = text.replace('يي', 'ي')\n",
        "    text = text.replace('اا', 'ا')\n",
        "    #remove longation\n",
        "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    text = re.sub(\"گ\", \"ك\", text)\n",
        "    \n",
        "    for i in range(0, len(search)):\n",
        "        text = text.replace(search[i], replace[i])\n",
        "    \n",
        "    text = text.strip()\n",
        "    stemmer = nltk.ISRIStemmer()\n",
        "    word_list = nltk.word_tokenize(text)\n",
        "    #remove arabic stopwords\n",
        "    word_list = [ w for w in word_list if not w in stop_words ]\n",
        "    #remove digits\n",
        "    word_list = [ w for w in word_list if not w.isdigit() ]\n",
        "    #stemming\n",
        "    word_list = [stemmer.stem(w) for w in  word_list]\n",
        "    text= ' '.join(word_list) \n",
        "    \n",
        "    return text"
      ],
      "metadata": {
        "id": "41M07qv34KL5"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
        "stop_words = stopwords.words()\n",
        "\n",
        "data['sent'] = data['sent'].apply(clean_text)\n",
        "data.head(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "cjUNPiFx4TB9",
        "outputId": "789ecc6b-04bf-4621-8107-5168fb78d272"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-aeb64c06-f706-4652-aae3-18de43216044\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sent</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>طول حيت لمس اي تغر حتي قدم حكم فل نقف بجن بصت</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>نتج رءع وثم نسب جمل</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>كلن ابن كير تفق معا يدر</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>وفق الل لول اخر حقش كينش حسن منك</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>لنه بكل بسط رءس حكم يعت عاق دخل بيت الل ميز حسن</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aeb64c06-f706-4652-aae3-18de43216044')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-aeb64c06-f706-4652-aae3-18de43216044 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-aeb64c06-f706-4652-aae3-18de43216044');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              sent  y\n",
              "0    طول حيت لمس اي تغر حتي قدم حكم فل نقف بجن بصت  1\n",
              "1                              نتج رءع وثم نسب جمل  1\n",
              "2                          كلن ابن كير تفق معا يدر  1\n",
              "3                 وفق الل لول اخر حقش كينش حسن منك  1\n",
              "4  لنه بكل بسط رءس حكم يعت عاق دخل بيت الل ميز حسن  1"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "split data into train and test"
      ],
      "metadata": {
        "id": "YleI0BZckll4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature = data.sent\n",
        "target = data.y"
      ],
      "metadata": {
        "id": "uCLAjpRqhTmM"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(feature, target, test_size =.2, random_state=100)\n"
      ],
      "metadata": {
        "id": "ZgW0mKcVku29"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXVU2ezNLiCU",
        "outputId": "bc108079-73c6-4018-9082-68d46b692500"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1600,)\n",
            "(400,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Modeling*"
      ],
      "metadata": {
        "id": "BSPBsNDdFqh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will create 3 models for comparison \n",
        "\n",
        "\n",
        "1.   SVC\n",
        "2.   LSTM\n",
        "3.   CNN\n",
        "\n"
      ],
      "metadata": {
        "id": "kKUpNwVtR4Mk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **I started by crating a TF IDF + SVC pipeline as a baseline**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BA7q37UeGJp9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n"
      ],
      "metadata": {
        "id": "a_7Ck_6_lu81"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create the  pipeline\n",
        "pipe = make_pipeline(TfidfVectorizer(), SVC())\n",
        "#Hyperparameter tuning using  GridSearch cross validation\n",
        "param_grid = {'svc__kernel': ['rbf', 'linear', 'poly'],\n",
        "             'svc__gamma': [0.1, 1, 10, 100],\n",
        "             'svc__C': [0.1, 1, 10, 100]}\n",
        "\n",
        "svc_model = GridSearchCV(pipe, param_grid, cv=3)\n",
        "svc_model.fit(X_train, Y_train)\n",
        "\n",
        "prediction = svc_model.predict(X_test)\n",
        "#evaluation\n",
        "print(f\"Accuracy score is {accuracy_score(Y_test, prediction):.2f}\")\n",
        "print(classification_report(Y_test, prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zV0PdNcl2au",
        "outputId": "b6de5dda-a759-4d01-bbfc-e701fd114269"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score is 0.84\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.91      0.85       204\n",
            "           1       0.89      0.78      0.83       196\n",
            "\n",
            "    accuracy                           0.84       400\n",
            "   macro avg       0.85      0.84      0.84       400\n",
            "weighted avg       0.85      0.84      0.84       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **CNN Model**"
      ],
      "metadata": {
        "id": "VCgjyX9JSNUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will use Keras Tokenizer to split each word in a sentence. Then, in order to get a sequential representation of each row we use texts_to_sequences method. after that, we check the first five entries and there sequential representation, where each word is represented by a number."
      ],
      "metadata": {
        "id": "3x5F3m2RQ1_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=100000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "sequences = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "for x in X_train[:5]:\n",
        "    print (x)    \n",
        "sequences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ6kwWgapLjU",
        "outputId": "b008d75b-ac00-4fba-99a5-485e647823bb"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "يحق ان يخد حرب علي تهل ربق\n",
            "وفق ان شاء الل\n",
            "الل كمل برك الل\n",
            "اظن ان شعب غرب تبع رمج فظل غلق وسع نظر\n",
            "عار عار\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1729, 4, 863, 195, 2, 214, 1730],\n",
              " [10, 4, 21, 1],\n",
              " [1, 284, 19, 1],\n",
              " [683, 4, 11, 3, 258, 123, 1132, 684, 563, 27],\n",
              " [237, 237]]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can check the max length of rows in our corpus for padding.\n",
        "\n"
      ],
      "metadata": {
        "id": "qumhCP7GRAQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "length = []\n",
        "for x in X_train:\n",
        "    length.append(len(x.split()))\n",
        "max(length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT_IxMnpgssM",
        "outputId": "75e4808f-6639-4945-a259-e427801d4d5e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "70"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can say that the maximum length to be 80\n",
        "\n"
      ],
      "metadata": {
        "id": "TpbhtTrQRFnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_seq = pad_sequences(sequences, maxlen=80)\n",
        "x_train_seq[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SL42h63hVjE",
        "outputId": "c00f78ee-b4be-48bc-9624-836582b09a0f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0, 1773,    4,  895,  188,  208,\n",
              "           2,  254, 1774],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   11,\n",
              "           4,   22,    1],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n",
              "         305,   15,    1],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,  712,    4,   12,    3,  278,  135, 1166,  713,\n",
              "        1167,  589,   28],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,  255,  255]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After checking, we can see that all the data transformed to have the same length of 80.\n",
        "\n",
        "We do the same thing to the test set."
      ],
      "metadata": {
        "id": "V9Yz87D7RKrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
        "x_test_seq = pad_sequences(sequences_test, maxlen=80)\n",
        "x_test_seq[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUsF39umhaPg",
        "outputId": "4d33824d-e99d-4f3b-be5c-5a9c69928357"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,  223,   57,\n",
              "         377,  887,  135,  360, 1486, 3636,  171,  576,  518,  449,   82,\n",
              "         308,  387,  135,  192,  518, 1883, 2284, 2672, 1401,  718,  710,\n",
              "         718,    2,   12],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,  515, 1426],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,  344,   80,  281,\n",
              "         139, 2884,  139],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,   21, 2951,    2,  689,\n",
              "        1467, 1220, 1309],\n",
              "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          72,   15,   14,   30,  254,  173, 1558,  372,   90,    5,   69,\n",
              "          18,    9,   70]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I defined a CNN using an embedding layer of 200x80 dimension as an input with 100000 as a max feature, then add to our 1D Convolutional layer 100x2000 filters, then add Global Max Pooling layer which will extract the maximum value from each filter. Finally, the output will be a one-dimensional vector with length equal to the number of the filters."
      ],
      "metadata": {
        "id": "CWE5BXpiRSa0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, GlobalMaxPooling1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "\n",
        "model_cnn = Sequential()\n",
        "\n",
        "e = Embedding(100000, 100, input_length=80)\n",
        "model_cnn.add(e)\n",
        "model_cnn.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
        "model_cnn.add(GlobalMaxPooling1D())\n",
        "model_cnn.add(Dense(256, activation='relu'))\n",
        "model_cnn.add(Dense(1, activation='sigmoid'))\n",
        "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_cnn.fit(x_train_seq, Y_train, validation_data=(x_test_seq, Y_test), epochs=5, batch_size=32, verbose=2)\n",
        "score,acc = model_cnn.evaluate(x_test_seq, Y_test, verbose = 2, batch_size = 32)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOtDm0swkeDe",
        "outputId": "172aa0f3-4521-4397-8390-e67c6b4acc9b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "50/50 - 5s - loss: 0.6666 - accuracy: 0.6294 - val_loss: 0.5866 - val_accuracy: 0.7800 - 5s/epoch - 97ms/step\n",
            "Epoch 2/5\n",
            "50/50 - 3s - loss: 0.3294 - accuracy: 0.8975 - val_loss: 0.3512 - val_accuracy: 0.8575 - 3s/epoch - 62ms/step\n",
            "Epoch 3/5\n",
            "50/50 - 3s - loss: 0.0898 - accuracy: 0.9719 - val_loss: 0.3942 - val_accuracy: 0.8450 - 3s/epoch - 61ms/step\n",
            "Epoch 4/5\n",
            "50/50 - 3s - loss: 0.0206 - accuracy: 0.9969 - val_loss: 0.4095 - val_accuracy: 0.8725 - 3s/epoch - 60ms/step\n",
            "Epoch 5/5\n",
            "50/50 - 3s - loss: 0.0069 - accuracy: 0.9994 - val_loss: 0.4705 - val_accuracy: 0.8500 - 3s/epoch - 61ms/step\n",
            "13/13 - 0s - loss: 0.4705 - accuracy: 0.8500 - 58ms/epoch - 4ms/step\n",
            "score: 0.47\n",
            "acc: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After training, we get the above accuracy, which seems a better result from the model I've run in previous kernel SVC+TF-IDF."
      ],
      "metadata": {
        "id": "qiEKDv7aRgzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***LSTM Model***\n"
      ],
      "metadata": {
        "id": "ICCXEWvARySG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this model will use a single LSTM layer preceded by an embedding layer with 100000 as a max feature and 128 dimensions of each word in a sequence, then followed by a dense layer with softmax function."
      ],
      "metadata": {
        "id": "Bl-c9G5cSmog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import SpatialDropout1D, LSTM, Dropout\n",
        "\n",
        "model_lstm = Sequential()\n",
        "\n",
        "model_lstm.add(Embedding(100000, 128))\n",
        "model_lstm.add(SpatialDropout1D(0.4))\n",
        "model_lstm.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model_lstm.add(Dense(1,activation='softmax'))\n",
        "model_lstm.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "print(model_lstm.summary())\n",
        "model_lstm.fit(x_train_seq,Y_train, epochs = 10, batch_size=32, verbose = 2)\n",
        "score,acc = model_lstm.evaluate(x_test_seq, Y_test, verbose = 2, batch_size = 32)\n",
        "print(\"score: %.2f\" % (score))\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W46eq9DAkgot",
        "outputId": "62a5a037-8602-4c86-bc51-66a7f97732fd"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_8 (Embedding)     (None, None, 128)         12800000  \n",
            "                                                                 \n",
            " spatial_dropout1d_2 (Spatia  (None, None, 128)        0         \n",
            " lDropout1D)                                                     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,931,713\n",
            "Trainable params: 12,931,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/10\n",
            "50/50 - 14s - loss: 0.6481 - accuracy: 0.5025 - 14s/epoch - 276ms/step\n",
            "Epoch 2/10\n",
            "50/50 - 11s - loss: 0.4493 - accuracy: 0.5025 - 11s/epoch - 216ms/step\n",
            "Epoch 3/10\n",
            "50/50 - 11s - loss: 0.2071 - accuracy: 0.5025 - 11s/epoch - 217ms/step\n",
            "Epoch 4/10\n",
            "50/50 - 11s - loss: 0.1108 - accuracy: 0.5025 - 11s/epoch - 217ms/step\n",
            "Epoch 5/10\n",
            "50/50 - 11s - loss: 0.0756 - accuracy: 0.5025 - 11s/epoch - 219ms/step\n",
            "Epoch 6/10\n",
            "50/50 - 11s - loss: 0.0448 - accuracy: 0.5025 - 11s/epoch - 219ms/step\n",
            "Epoch 7/10\n",
            "50/50 - 11s - loss: 0.0324 - accuracy: 0.5025 - 11s/epoch - 217ms/step\n",
            "Epoch 8/10\n",
            "50/50 - 11s - loss: 0.0193 - accuracy: 0.5025 - 11s/epoch - 220ms/step\n",
            "Epoch 9/10\n",
            "50/50 - 11s - loss: 0.0150 - accuracy: 0.5025 - 11s/epoch - 222ms/step\n",
            "Epoch 10/10\n",
            "50/50 - 11s - loss: 0.0075 - accuracy: 0.5025 - 11s/epoch - 217ms/step\n",
            "13/13 - 1s - loss: 0.6089 - accuracy: 0.4900 - 589ms/epoch - 45ms/step\n",
            "score: 0.61\n",
            "acc: 0.49\n"
          ]
        }
      ]
    }
  ]
}